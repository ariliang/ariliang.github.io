<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>基于GPT2对话系统实现 - Ari's home</title><link rel=icon type=image/png href=favicon.jpeg><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content="基于GPT2对话系统实现"><meta property="og:description" content="对话系统、对话生成，就是要给定一句话，系统根据历史上下文对这句话进行回复"><meta property="og:type" content="article"><meta property="og:url" content="https://ariliang.github.io/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-09T21:43:14+08:00"><meta property="article:modified_time" content="2021-05-09T21:43:14+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="基于GPT2对话系统实现"><meta name=twitter:description content="对话系统、对话生成，就是要给定一句话，系统根据历史上下文对这句话进行回复"><link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://ariliang.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://ariliang.github.io/css/main.css><link rel=stylesheet type=text/css href=https://ariliang.github.io/css/dark.css media="(prefers-color-scheme: dark)"><script src=https://ariliang.github.io/js/feather.min.js></script><script src=https://ariliang.github.io/js/main.js></script></head><body><div class="container wrapper post"><div class=header><base href=https://ariliang.github.io/><h1 class=site-title><a href=https://ariliang.github.io/>Ari's home</a></h1><div class=site-description><nav class="nav social"><ul class=flat><a href=https://github.com/ariliang title=Github><i data-feather=github></i></a></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>Posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></nav></div><div class=post-header><h1 class=title>基于GPT2对话系统实现</h1><div class=meta>Posted at &mdash; May 9, 2021</div></div><div class=post-toc><div class=toc-header>CATALOG</div><ul class=toc-h2><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e7%ae%80%e4%bb%8b class=toc-link>简介</a></li><ul class=toc-h3><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#why-gpt2 class=toc-link>Why GPT2?</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e6%9c%af%e8%af%ad class=toc-link>术语</a></li></ul><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e4%bb%bb%e5%8a%a1%e7%90%86%e8%a7%a3 class=toc-link>任务理解</a></li><ul class=toc-h3><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e8%ae%ad%e7%bb%83%e9%9b%86 class=toc-link>训练集</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e8%be%93%e5%85%a5 class=toc-link>输入</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e8%be%93%e5%87%ba class=toc-link>输出</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e9%87%87%e6%a0%b7%e6%96%b9%e5%bc%8f class=toc-link>采样方式</a></li></ul><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e5%8c%bb%e7%96%97%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f%e6%94%b9%e8%bf%9b class=toc-link>医疗对话系统改进</a></li></div><div class=markdown><h2 id=简介>简介</h2><p>对话系统、对话生成，就是要给定一句话，系统根据历史上下文对这句话进行回复。这是一个NLG(Natural Language Generation)任务，或者LM(Language Modeling)语言建模。</p><p>所谓语言建模，就是给定一个句子序列$x_1, x_2, \dots, x_{i-1}$，预测$x_i$, 给出预测的概率值。</p><p>$$
prob = p(x_{i} | x_{\lt i})
$$</p><p>或者，给定$x_1, \dots, x_{k-1}, x_{k+1}, \dots, x_n$，预测$x_k$</p><p>$$
prob = p(x_k | x_{i \ne k})
$$</p><p>句子生成的过程，其实也是训练模型的过程，所以叫语言建模。</p><h3 id=why-gpt2>Why GPT2?</h3><p>GPT2是一种自回归(Auto Regression, AR)模型，也称CLM(Casual Language Model)模型，两种说法等同，类似于RNN与LSTM。所谓自回归，就是会把输出附加到输入末尾，得到下一次的输入，如此往复直到符合结束条件，如达到句子最大长度、结束标志。这种模型是单向的，从左至右，所以没法知道后面的词对当前词的影响。</p><p>相对于自回归模型，另一种是遮罩模型MLM(Masked Language Modeling)，也称AE(Auto Encoding)模型，两种说法等同。这种模型会随机遮盖句子一部分，然后预测被Mask掉的词，如BERT。采用Self Attention机制，每两个字或词之间可以相互联系，所以是双向的，对于理解比较适合，常用来提取特征，然后做句子分类，情感分析之类的任务。</p><p>自回归模型比较符合人类语言从左往右的特性，特别适合于生成<strong>流畅的</strong>、<strong>符合人类语言</strong>的句子，但是生成的文本不可控，所以后续需要想办法！</p><h3 id=术语>术语</h3><ul><li>CLM, Casual Language Modeling: 自回归语言建模，把输入附加到输入末尾，得到下一次的输入</li><li>MLM, Masked Language Modeling: 遮罩的语言建模，随机遮罩句子一部分，对其进行预测</li><li>AR, Auto Regression: 自回归，同CLM</li><li>AE, Auto Encoding: 自编码，同MLM。因为采用了Self Attention机制，每两个字或词之间都可以相互联系，所以跟位置没有关系。而句子是有顺序的，要区分位置就引入了位置编码。这种方式类似于编码的过程，所以叫做自编码模型</li><li>Self Attention: 自注意机制，可以对句子进行双向理解</li><li>NLG, Natural Language Generation: 自然语言生成</li><li>Seq2Seq, Sequence to Sequncce: 从一个序列到另一个序列，如机器翻译，文本生成</li><li>token: 字或词，是模型处理的基本单位。要先把句子token化，然后转换成对应的id才能输入进模型，进行处理</li></ul><p><a href=https://huggingface.co/transformers/glossary.html>Ref: Terms</a></p><h2 id=任务理解>任务理解</h2><p>我们可以对一个历史对话进行拼接(Concatenate)，形成一个长句子输入进模型，模型每次输出一个字；再添加到长句子末尾，再输入进模型。如此往复直到达到最大长度max_len，或者结束遇到&lt;eos>，如：</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span>今天好点了吗？
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span>一天比一天严重
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3</span>吃药不管用，去打一针。别拖着
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">5</span>转换为：
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">6</span>&lt;cls&gt;今天好点了吗？&lt;sep&gt;一天比一天严重&lt;sep&gt;吃药不管用，去打一针。别拖着&lt;sep&gt;&lt;pad&gt;...
</code></pre></div><ul><li>&lt;cls>(Classfier)：表示区分，这个是一个整体</li><li>&lt;sep>(Seperator)：表示分隔符，区分不同句子</li><li>&lt;eos>(End of Sentence)：结束符</li><li>&lt;pad>(Padding)：用于填充，因为要批处理，每个输入向量长度要一致</li></ul><h3 id=训练集>训练集</h3><p>GPT2是使用大量<strong>无标注数据</strong>训练的自回归模型，将自然语言输入进模型就可以进行建模(Language Modeling)了，输出就是下一个字的概率，所以训练集可以这样构造：</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1</span># train.txt
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2</span># dialog 1
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3</span>conva
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4</span>convb
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5</span>conva
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6</span>...
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8</span># dialog 2
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9</span>conva
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10</span>convb
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">11</span>...
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">12</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">13</span>...
</code></pre></div><p><a href=https://huggingface.co/transformers/model_doc/gpt2.html#>Ref: OpenAI GPT2</a></p><h3 id=输入>输入</h3><h3 id=输出>输出</h3><h3 id=采样方式>采样方式</h3><p>将一个长句子输入进模型后，会得到下一个字的概率，有不同的策略对这个字进行采样(Sampling)。若想要生成的句子最符合训练集情况，就选择概率最大，但这样会生成重复的句子，对于聊天机器人可能不太好；想要生成的句子具有多样性，可选取其他采样方式。</p><p><strong>Greedy Search</strong>，简单地取最大概率</p><p><strong>Top-K</strong>，选取前k个概率最大的样本，再随机取一个样本</p><p><strong>Top-P</strong>，选取概率超过一定阈值的样本，如0.9，再从这些样本中随机取一个</p><p><strong>Beam Seach</strong>，设置一个N值代表N个样本，对当前生成的选择概率最大的N个样本；在下一轮生成时，计算这N个样本与词表所有词组合后的概率值，再选择概率前N个值，如此往复，最后生成概率最大的N个句子</p><p><strong>Temperture</strong>，$p_j \propto \frac{\exp(p_j / T)}{\sum_i \exp(p_i/T)}$，通过调节Temperture可对全体概率进行平滑或者锐化</p><p><strong>Penalty</strong>，每次生成一个词后，我们可以添加一个惩罚项，对这个词的概率进行处理，使其减少重复，如 $p_j = \frac{\exp(p_j / T / Penalty)}{\sum_i \exp(p_i/T/Penalty)}$, where $i, j$ has been sampled</p><blockquote><p>这些采样方式通常可以组合，比如先选取Top-P取概率超过一定阈值的样本，再使用Top-K取前K个，再随机选取一个</p></blockquote><h2 id=医疗对话系统改进>医疗对话系统改进</h2><p>上述的对话系统有一个问题，简单地拼接对话历史，模型学习到了语言特征，但这个学到的语言特征包括A和B的，生成的句子也可能是A或B说的话，可以简单地认为A与B的角色是等同的，称为<strong>闲聊系统</strong>。而Task-Oriented聊天机器人，A和B的角色不一样，是<strong>问跟答</strong>的关系。</p><p>这个问题可以通过正确地构造训练集来实现。</p></div><div class=post-tags><nav class="nav tags"><ul class=flat><li><a href=/tags/gpt2>GPT2</a></li><li><a href=/tags/nlp>NLP</a></li><li><a href=/tags/dialogue-system>Dialogue System</a></li></ul></nav></div></div><div class="footer wrapper"><nav class=nav><div><a href=https://github.com/vividvilla/ezhil>Ezhil theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></nav></div><script>feather.replace()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/katex.min.css integrity=sha384-r/BYDnh2ViiCwqZt5VJVWuADDic3NnnTIEOv4hOh05nSfB6tjWpKmn1kUHOVkMXc crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/katex.min.js integrity=sha384-zDIgORxjImEWftZXZpWLs2l57fMX9B3yWFPN5Ecabe211Hm5ZG/OIz2b07DYPUcH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body></html>