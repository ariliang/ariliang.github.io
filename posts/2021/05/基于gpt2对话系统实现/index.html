<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>基于GPT2对话系统实现 - Ari's home</title><link rel=icon type=image/png href=favicon.jpeg><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content="基于GPT2对话系统实现"><meta property="og:description" content="对话系统、对话生成，就是给定一句话，系统根据历史上下文对这句话进行回复"><meta property="og:type" content="article"><meta property="og:url" content="https://ariliang.github.io/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-09T21:43:14+08:00"><meta property="article:modified_time" content="2021-05-09T21:43:14+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="基于GPT2对话系统实现"><meta name=twitter:description content="对话系统、对话生成，就是给定一句话，系统根据历史上下文对这句话进行回复"><link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://ariliang.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://ariliang.github.io/css/main.css><link rel=stylesheet type=text/css href=https://ariliang.github.io/css/dark.css media="(prefers-color-scheme: dark)"><script src=https://ariliang.github.io/js/feather.min.js></script><script src=https://ariliang.github.io/js/main.js></script></head><body><div class="container wrapper post"><div class=header><base href=https://ariliang.github.io/><h1 class=site-title><a href=https://ariliang.github.io/>Ari's home</a></h1><div class=site-description><nav class="nav social"><ul class=flat><a href=https://github.com/ariliang title=Github><i data-feather=github></i></a></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>Posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></nav></div><div class=post-header><h1 class=title>基于GPT2对话系统实现</h1><div class=meta>Posted at &mdash; May 9, 2021</div></div><div class=post-toc><div class=toc-header>CATALOG</div><ul class=toc-h2><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e7%ae%80%e4%bb%8b class=toc-link>简介</a></li><ul class=toc-h3><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#nlp%e4%b8%80%e8%88%ac%e8%bf%87%e7%a8%8b class=toc-link>NLP一般过程</a></li><ul class=toc-h4><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#fig1-the-transformer class=toc-link>Fig.1 The Transformer</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#fig2-encoder class=toc-link>Fig.2 Encoder</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#fig3-decoder class=toc-link>Fig.3 Decoder</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#fig4-self-attention class=toc-link>Fig.4 Self-Attention</a></li></ul><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1 class=toc-link>语言建模</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#why-gpt2 class=toc-link>Why GPT2?</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e6%9c%af%e8%af%ad class=toc-link>术语</a></li></ul><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e4%bb%bb%e5%8a%a1%e7%90%86%e8%a7%a3 class=toc-link>任务理解</a></li><ul class=toc-h3><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e8%ae%ad%e7%bb%83%e9%9b%86 class=toc-link>训练集</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e9%a1%b9%e7%9b%ae%e7%bb%93%e6%9e%84 class=toc-link>项目结构</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e8%be%93%e5%85%a5 class=toc-link>输入</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e8%be%93%e5%87%ba class=toc-link>输出</a></li><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e9%87%87%e6%a0%b7%e6%96%b9%e5%bc%8f class=toc-link>采样方式</a></li></ul><li><a href=/posts/2021/05/%E5%9F%BA%E4%BA%8Egpt2%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/#%e5%8c%bb%e7%96%97%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f%e6%94%b9%e8%bf%9b class=toc-link>医疗对话系统改进</a></li></div><div class=markdown><h2 id=简介>简介</h2><h3 id=nlp一般过程>NLP一般过程</h3><p>自从预训练模型BERT(Bidirectional Encoder Representation for Transformer)诞生后，当前大多采用大规模通用语料如网页、维基百科，来训练预训练模型(Pretrained Model)，然后再根据下游任务对预训练模型进行精调(Fine Tuning)，这样的方式在许多任务上的表现非常好。</p><blockquote><p>精调是个宽泛的说法，通常包括几种方式：不改变模型，只改变数据；根据任务只改变输出层，模型大体不动；对模型做出修改等</p></blockquote><p>这里有几个注意的点：</p><ol><li>预训练模型普遍采用编码解码(Encoder-Decoder)模式，这也是一个广泛的说法。广义上指对数据$x$进行处理变成了中间态$z$：如把原始数据做Embedding、对数据处理后得到hidden output，相当于对数据进行了编码；然后对这些编码结果再进行处理，变成了我们想要的结果$y$，这类似与解码的过程。</li><li>狭义上指使用了Transformer结构的模型，它包含了具体的Encoder跟Decoder部分，而BERT使用了Encoder部分，GPT使用了Decoder部分，所以上面说BERT含义大概是是双向的Transformer的Encoder表示<figure><img src=images/%e5%9f%ba%e4%ba%8eGPT2%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f%e5%ae%9e%e7%8e%b0/transformer-encoder-decoder.png alt=transformer width=75%><figcaption><h4>Fig.1 The Transformer</h4></figcaption></figure></li><li>Transformer采用注意力机制(Attention)，所谓注意力机制就是在一个句子中，基本的单位如字、词对另外的字或词是否可见、是否能注意到其它字词，相互是否能得到一个影响，如用概率大小来表示。注意到Encoder跟Decoder的Attention的差别<figure><img src=images/%e5%9f%ba%e4%ba%8eGPT2%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f%e5%ae%9e%e7%8e%b0/transformer-encoder-block-2.png alt=Encoder width=75%><figcaption><h4>Fig.2 Encoder</h4></figcaption></figure><figure><img src=images/%e5%9f%ba%e4%ba%8eGPT2%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f%e5%ae%9e%e7%8e%b0/transformer-decoder-block-2.png alt=Decoder width=75%><figcaption><h4>Fig.3 Decoder</h4></figcaption></figure></li><li>BERT使用了Self-Attention，而GPT使用了遮罩的注意力机制(Masked Self-Attetion)，只有后面的能注意到前面的<figure><img src=images/%e5%9f%ba%e4%ba%8eGPT2%e5%af%b9%e8%af%9d%e7%b3%bb%e7%bb%9f%e5%ae%9e%e7%8e%b0/self-attention-and-masked-self-attention.png alt=Self-Attention width=75%><figcaption><h4>Fig.4 Self-Attention</h4></figcaption></figure></li></ol><p><a href=http://jalammar.github.io/illustrated-gpt2/>Ref: Illustrated GPT2</a></p><h3 id=语言建模>语言建模</h3><p>对话系统、对话生成，就是要给定一句话，系统根据历史上下文对这句话进行回复。这是一个NLG(Natural Language Generation)任务，或者LM(Language Modeling)语言建模。</p><p>所谓语言建模，就是给定一个句子序列$x_1, x_2, \dots, x_{i-1}$，预测$x_i$, 给出预测的概率值。</p><p>$$
prob = p(x_{i} | x_{\lt i})
$$</p><p>或者，给定$x_1, \dots, x_{k-1}, x_{k+1}, \dots, x_n$，预测$x_k$</p><p>$$
prob = p(x_k | x_{i \ne k})
$$</p><p>句子生成的过程，其实也是训练模型的过程，所以叫语言建模。</p><h3 id=why-gpt2>Why GPT2?</h3><p>GPT2是一种自回归(Auto Regression, AR)模型，也称CLM(Casual Language Model)模型，两种说法等同，类似于RNN与LSTM。所谓自回归，就是会把输出附加到输入末尾，得到下一次的输入，如此往复直到符合结束条件，如达到句子最大长度、结束标志。这种模型是单向的，从左至右，所以没法知道后面的词对当前词的影响。</p><p>相对于自回归模型，另一种是遮罩模型MLM(Masked Language Modeling)，也称AE(Auto Encoding)模型，两种说法等同。这种模型会随机遮盖句子一部分，然后预测被Mask掉的词，如BERT。采用Self Attention机制，每两个字或词之间可以相互联系，所以是双向的，对于理解比较适合，常用来提取特征，然后做句子分类，情感分析之类的任务。</p><p>自回归模型比较符合人类语言从左往右的特性，特别适合于生成<strong>流畅的</strong>、<strong>符合人类语言</strong>的句子，但是生成的文本不可控，所以后续需要想办法！</p><h3 id=术语>术语</h3><ul><li>CLM, Casual Language Modeling: 自回归语言建模，把输入附加到输入末尾，得到下一次的输入</li><li>MLM, Masked Language Modeling: 遮罩的语言建模，随机遮罩句子一部分，对其进行预测</li><li>AR, Auto Regression: 自回归，同CLM</li><li>AE, Auto Encoding: 自编码，同MLM。因为采用了Self Attention机制，每两个字或词之间都可以相互联系，所以跟位置没有关系。而句子是有顺序的，要区分位置就引入了位置编码。这种方式类似于编码的过程，所以叫做自编码模型</li><li>Self Attention: 自注意机制，可以对句子进行双向理解</li><li>NLG, Natural Language Generation: 自然语言生成</li><li>Seq2Seq, Sequence to Sequncce: 从一个序列到另一个序列，如机器翻译，文本生成</li><li>token: 字或词，是模型处理的基本单位。要先把句子token化，然后转换成对应的id才能输入进模型，进行处理</li></ul><p><a href=https://huggingface.co/transformers/glossary.html>Ref: Terms</a></p><h2 id=任务理解>任务理解</h2><p>我们可以对一个历史对话进行拼接(Concatenate)，形成一个长句子输入进模型，模型每次输出一个字；再添加到长句子末尾，再输入进模型。如此往复直到达到最大长度max_len，或者结束遇到&lt;eos>，如：</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span>今天好点了吗？
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span>一天比一天严重
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3</span>吃药不管用，去打一针。别拖着
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">5</span>转换为：
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">6</span>&lt;cls&gt;今天好点了吗？&lt;sep&gt;一天比一天严重&lt;sep&gt;吃药不管用，去打一针。别拖着&lt;sep&gt;&lt;pad&gt;...
</code></pre></div><ul><li>&lt;cls>(Classfier)：表示区分，这个是一个整体</li><li>&lt;sep>(Seperator)：表示分隔符，区分不同句子</li><li>&lt;eos>(End of Sentence)：结束符</li><li>&lt;pad>(Padding)：用于填充，因为要批处理，每个输入向量长度要一致</li></ul><h3 id=训练集>训练集</h3><p>GPT2是使用大量<strong>无标注数据</strong>训练的自回归模型，将自然语言输入进模型就可以进行建模(Language Modeling)了，输出就是下一个字的概率，所以训练集可以这样构造：</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1</span># train.txt
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2</span># dialog 1
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3</span>conva
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4</span>convb
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5</span>conva
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6</span>...
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8</span># dialog 2
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9</span>conva
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10</span>convb
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">11</span>...
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">12</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">13</span>...
</code></pre></div><p><a href=https://huggingface.co/transformers/model_doc/gpt2.html#>Ref: OpenAI GPT2</a></p><h3 id=项目结构>项目结构</h3><p>因为代码里边参数的注释非常详细了，所以先只介绍整个大概的流程，以下是整个项目的结构</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1</span><span style=color:#000>project</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2</span><span style=color:#000>├──</span> <span style=color:#000>config</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3</span><span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>args</span><span style=color:#000>.</span><span style=color:#000>py</span>                     <span style=color:#177500># 所有训练参数</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4</span><span style=color:#000>├──</span> <span style=color:#000>data</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>test_tokenized</span><span style=color:#000>.</span><span style=color:#000>txt</span>          <span style=color:#177500># 已处理的测试集</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>test</span><span style=color:#000>.</span><span style=color:#000>txt</span>                    <span style=color:#177500># 原始测试集</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>train_tokenized</span><span style=color:#000>.</span><span style=color:#000>txt</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8</span><span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>train</span><span style=color:#000>.</span><span style=color:#000>txt</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9</span><span style=color:#000>├──</span> <span style=color:#000>logs</span>                            <span style=color:#177500># 日志目录</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10</span><span style=color:#000>├──</span> <span style=color:#000>model</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">11</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>model_config_</span><span style=color:#000>....</span><span style=color:#000>json</span>       <span style=color:#177500># 模型的配置信息</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">12</span><span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>vocab_small</span><span style=color:#000>.</span><span style=color:#000>txt</span>             <span style=color:#177500># 字典</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">13</span><span style=color:#000>├──</span> <span style=color:#000>output</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">14</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>dialogue_model</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">15</span><span style=color:#000>│</span>   <span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>model_epoch1</span>            <span style=color:#177500># 训练好的模型;</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">16</span><span style=color:#000>│</span>   <span style=color:#000>│</span>       <span style=color:#000>├──</span> <span style=color:#000>config</span><span style=color:#000>.</span><span style=color:#000>json</span>         <span style=color:#177500># 包含模型本身与配置文件</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">17</span><span style=color:#000>│</span>   <span style=color:#000>│</span>       <span style=color:#000>└──</span> <span style=color:#000>pytorch_model</span><span style=color:#000>.</span><span style=color:#000>bin</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">18</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>mmi_model</span>                   <span style=color:#177500># 若开起了mmi，则输出mmi模型</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">19</span><span style=color:#000>│</span>   <span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>model_epoch1</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">20</span><span style=color:#000>│</span>   <span style=color:#000>│</span>       <span style=color:#000>├──</span> <span style=color:#000>config</span><span style=color:#000>.</span><span style=color:#000>json</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">21</span><span style=color:#000>│</span>   <span style=color:#000>│</span>       <span style=color:#000>└──</span> <span style=color:#000>pytorch_model</span><span style=color:#000>.</span><span style=color:#000>bin</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">22</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>sample</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">23</span><span style=color:#000>│</span>   <span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>output_test</span><span style=color:#000>.</span><span style=color:#000>txt</span>         <span style=color:#177500># 测试集输入进模型的输出</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">24</span><span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>tensorboard_summary</span>         <span style=color:#177500># tensorborad输出</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">25</span><span style=color:#000>├──</span> <span style=color:#000>src</span>                             <span style=color:#177500># 可以用来可视化训练过程</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">26</span><span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>chitchat_demo</span><span style=color:#000>.</span><span style=color:#000>png</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">27</span><span style=color:#000>├──</span> <span style=color:#000>utils</span>                           <span style=color:#177500># 工具类</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">28</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>dataset</span><span style=color:#000>.</span><span style=color:#000>py</span>                  <span style=color:#177500># 包含</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">29</span><span style=color:#000>│</span>   <span style=color:#000>├──</span> <span style=color:#000>logger</span><span style=color:#000>.</span><span style=color:#000>py</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">30</span><span style=color:#000>│</span>   <span style=color:#000>└──</span> <span style=color:#000>preprocess</span><span style=color:#000>.</span><span style=color:#000>py</span>               <span style=color:#177500># 预处理数据集，具体是将原始文本tokenize，</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">31</span><span style=color:#000>├──</span> <span style=color:#000>generate_dialogue_subset</span><span style=color:#000>.</span><span style=color:#000>py</span>     <span style=color:#177500># 然后转换成id存储起来，以供后续快速加载</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">32</span><span style=color:#000>├──</span> <span style=color:#000>interact_mmi</span><span style=color:#000>.</span><span style=color:#000>py</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">33</span><span style=color:#000>├──</span> <span style=color:#000>interact</span><span style=color:#000>.</span><span style=color:#000>py</span>                     <span style=color:#177500># 交互式，就是跟系统对话的形式，测试模型</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">34</span><span style=color:#000>├──</span> <span style=color:#000>test</span><span style=color:#000>.</span><span style=color:#000>py</span>                         <span style=color:#177500># 测试，输入测试集，包含对话，输出最后的答复</span>
<span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">35</span><span style=color:#000>└──</span> <span style=color:#000>train</span><span style=color:#000>.</span><span style=color:#000>py</span>                        <span style=color:#177500># 训练</span>
</code></pre></div><h3 id=输入>输入</h3><h3 id=输出>输出</h3><h3 id=采样方式>采样方式</h3><p>将一个长句子输入进模型后，会得到下一个字的概率，有不同的策略对这个字进行采样(Sampling)。若想要生成的句子最符合训练集情况，就选择概率最大，但这样会生成重复的句子，对于聊天机器人可能不太好；想要生成的句子具有多样性，可选取其他采样方式。</p><ul><li><strong>Greedy Search</strong>，简单地取最大概率</li><li><strong>Top-K</strong>，选取前k个概率最大的样本，再随机取一个样本</li><li><strong>Top-P</strong>，选取概率超过一定阈值的样本，如0.9，再从这些样本中随机取一个</li><li><strong>Beam Seach</strong>，设置一个N值代表N个样本，对当前生成的选择概率最大的N个样本；在下一轮生成时，计算这N个样本与词表所有词组合后的概率值，再选择概率前N个值，如此往复，最后生成概率最大的N个句子</li><li><strong>Temperture</strong>，$p_j \propto \frac{\exp(p_j / T)}{\sum_i \exp(p_i/T)}$，通过调节Temperture可对全体概率进行平滑或者锐化</li><li><strong>Penalty</strong>，每次生成一个词后，我们可以添加一个惩罚项，对这个词的概率进行处理，使其减少重复，如 $p_j = \frac{\exp(p_j / T / Penalty)}{\sum_i \exp(p_i/T/Penalty)}$, where $i, j$ has been sampled</li></ul><blockquote><p>这些采样方式通常可以组合，比如先选取Top-P取概率超过一定阈值的样本，再使用Top-K取前K个，再随机选取某个样本</p></blockquote><h2 id=医疗对话系统改进>医疗对话系统改进</h2><p>上述的对话系统有一个问题，简单地拼接对话历史，模型学习到了语言特征，但这个学到的语言特征包括A和B的，生成的句子也可能是A或B说的话，可以简单地认为A与B的角色是等同的，称为<strong>闲聊系统</strong>。而Task-Oriented聊天机器人，A和B的角色不一样，是<strong>问跟答</strong>的关系。</p><p>这个问题可以通过正确地构造训练集来实现。</p></div><div class=post-tags><nav class="nav tags"><ul class=flat><li><a href=/tags/gpt2>GPT2</a></li><li><a href=/tags/dialogue-system>Dialogue System</a></li><li><a href=/tags/nlp>NLP</a></li></ul></nav></div></div><div class="footer wrapper"><nav class=nav><div><a href=https://github.com/vividvilla/ezhil>Ezhil theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></nav></div><script>feather.replace()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/katex.min.css integrity=sha384-r/BYDnh2ViiCwqZt5VJVWuADDic3NnnTIEOv4hOh05nSfB6tjWpKmn1kUHOVkMXc crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/katex.min.js integrity=sha384-zDIgORxjImEWftZXZpWLs2l57fMX9B3yWFPN5Ecabe211Hm5ZG/OIz2b07DYPUcH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body></html>